import json
<<<<<<< HEAD
=======
import os
import traceback
import re

import json
>>>>>>> origin/subtask-Ver

from zhipuai import ZhipuAI
import traceback
import re
import Sample

import api
import tools
from initial_prompt import initial_prompt
import Sample

function_map = {
    "calculate_uptime": api.calculate_uptime,
    "compute_operational_duration": api.compute_operational_duration,
    "get_table_data": api.get_table_data,
    "load_and_filter_data": api.load_and_filter_data,
    "calculate_total_energy": api.calculate_total_energy,
    "calculate_total_deck_machinery_energy": api.calculate_total_deck_machinery_energy,
    "query_device_parameter": api.query_device_parameter,
    "get_device_status_by_time_range": api.get_device_status_by_time_range,
    "calculate_total_energy_consumption": api.calculate_total_energy_consumption,
    "calculate_generator_energy_consumption": api.calculate_generator_energy_consumption,
    "check_ajia_angle": api.check_ajia_angle,
    "calculate_fuel_consumption": api.calculate_fuel_consumption,
    "calculate_fuel_consumption_weight": api.calculate_fuel_consumption_weight,
    "calculate_percent": api.calculate_percent,
    "calculate_theoretical_energy_output": api.calculate_theoretical_energy_output,
    "get_field_dict": api.get_field_dict,
    "sum_two": api.sum_two,
    "get_work_time": api.get_work_time,
    "find_missing_records": api.find_missing_records,
    "count_oscillations": api.count_oscillations,
    "find_min_value": api.find_min_value,
    "find_max_value": api.find_max_value,
    "find_avg_value": api.find_avg_value,
    "calculate_total_rudder_energy": api.calculate_total_rudder_energy,
    "count_swing_with_threshold": api.count_swing_with_threshold,
    "count_swing_with_rule": api.count_swing_with_rule,
    "calculate_time_difference": api.calculate_time_difference,
}


def create_chat_completion(messages, model="glm-4-plus"):
    print("å‘èµ·AIå¯¹è¯")
    client = ZhipuAI(api_key="6cf617672cae4afa9a280657a87beccb.m5ii3yJ1p3E42abg")
    response = client.chat.completions.create(
        model=model, stream=False, messages=messages
    )
    print("! AIå›å¤:", response.choices[0].message.content)
    return response


def choose_table(question):
    print("å‘AIæŸ¥è¯¢éœ€è¦çš„æ•°æ®è¡¨")
    with open("dict.json", "r", encoding="utf-8") as file:
        context_text = str(json.load(file))
    prompt = f"""æˆ‘æœ‰å¦‚ä¸‹æ•°æ®è¡¨ï¼š<{context_text}>
    ç°åœ¨éœ€è¦å›ç­”é—®é¢˜ï¼š{question}ã€‚
    è¯·ç»“åˆé—®é¢˜æè¿°éœ€è¦æŸ¥è¯¢å“ªäº›æ•°æ®è¡¨ã€‚
    ä»…è¿”å›éœ€è¦çš„æ•°æ®è¡¨åï¼Œæ— éœ€å±•ç¤ºåˆ†æè¿‡ç¨‹ã€‚
    è‹¥é—®é¢˜ä¸­æåˆ°Aæ¶åŠ¨ä½œ,åŒ…æ‹¬å…³æœºã€å¼€æœºã€Aæ¶æ‘†å‡ºã€ç¼†ç»³æŒ‚å¦¥ã€å¾æœè€…å‡ºæ°´ã€å¾æœè€…è½åº§ã€å¾æœè€…èµ·åŠã€å¾æœè€…å…¥æ°´ã€ç¼†ç»³è§£é™¤ã€Aæ¶æ‘†å›ï¼Œåˆ™ä½¿ç”¨Ajia_plc_1è¿™ä¸ªæ•°æ®è¡¨ã€‚
    è‹¥é—®é¢˜ä¸­æåˆ°æŠ˜è‡‚åŠè½¦åŠå°è‰‡åŠ¨ä½œ,åŒ…æ‹¬æŠ˜è‡‚åŠè½¦å…³æœºã€æŠ˜è‡‚åŠè½¦å¼€æœºã€å°è‰‡æ£€æŸ¥å®Œæ¯•ã€å°è‰‡å…¥æ°´ã€å°è‰‡è½åº§ï¼Œåˆ™ä½¿ç”¨device_13_11_meter_1311è¿™ä¸ªæ•°æ®è¡¨ã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    response = create_chat_completion(messages)
    return str(response.choices[0].message.content)


def glm4_create(max_attempts, messages, tools=None, model="glm-4-plus"):
    print("å‘èµ·AIå¯¹è¯")
    client = ZhipuAI(api_key="6cf617672cae4afa9a280657a87beccb.m5ii3yJ1p3E42abg")
    for attempt in range(max_attempts):
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
        )
        print("! å°è¯•æ¬¡æ•°ï¼š", attempt)
        if response.choices[0].message.content:
            print("! AIå›å¤:", response.choices[0].message.content)
        if response.choices and response.choices[0].message:
            return response
        else:
            continue
    return response


<<<<<<< HEAD
def get_answer_2(question, tools, api_look: bool = True):
=======
function_map = {
    "calculate_uptime": api.calculate_uptime,
    "compute_operational_duration": api.compute_operational_duration,
    "get_table_data": api.get_table_data,
    "load_and_filter_data": api.load_and_filter_data,
    "calculate_total_energy": api.calculate_total_energy,
    "calculate_total_deck_machinery_energy": api.calculate_total_deck_machinery_energy,
    "query_device_parameter": api.query_device_parameter,
    "get_device_status_by_time_range": api.get_device_status_by_time_range,
    "calculate_total_energy_consumption": api.calculate_total_energy_consumption,
    "calculate_generator_energy_consumption": api.calculate_generator_energy_consumption,
    "check_ajia_angle": api.check_ajia_angle,
    "calculate_fuel_consumption": api.calculate_fuel_consumption,
    "calculate_percent": api.calculate_percent,
    "calculate_theoretical_energy_output": api.calculate_theoretical_energy_output,
    "get_field_dict": api.get_field_dict,
    "sum_list": api.sum_list,
    "get_work_time": api.get_work_time,
    "find_missing_records": api.find_missing_records,
    "count_oscillations": api.count_oscillations,
}


def get_answer_3(question, tools, api_look: bool = True):
>>>>>>> origin/subtask-Ver
    filtered_tools = tools

    print(f"ğŸ”¹ ä»»åŠ¡æ‹†è§£å‰çš„é—®é¢˜ï¼š{question}")

    # **ä»»åŠ¡æ‹†è§£ï¼ˆCoTï¼‰**
    task_decomposition_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡æ‹†è§£åŠ©æ‰‹ã€‚è¯·é€æ­¥æ€è€ƒï¼Œæ‹†è§£æˆå¤šä¸ªå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ç¡®å®šéœ€è¦è°ƒç”¨çš„ APIã€‚
            å…ˆå¯ä»¥ç”¨ä½¿ç”¨çš„å·¥å…·æ—¶{filtered_tools}ã€‚
            åªéœ€è¦ä»»åŠ¡åˆ†è§£çš„JSONï¼Œä¸éœ€è¦å…¶ä»–å†…å®¹ã€‚
            è¿”å› JSON æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            {{
                "subtasks": [
                    {{"step": 1, "task": "æŸ¥è¯¢è®¾å¤‡çŠ¶æ€", "api": "get_device_status_by_time_range"}},
                    {{"step": 2, "task": "è®¡ç®—èƒ½è€—", "api": "calculate_total_energy"}}
                ]
            }}
            é—®é¢˜ï¼š{question}
            """
    print()
    task_response = glm4_create(3, [{"role": "user", "content": task_decomposition_prompt}], [])
    print("! ä»»åŠ¡æ‹†è§£ç»“æœ:", task_response.choices[0].message.content)

    try:
        messages = [
            {
                "role": "system",
                "content": initial_prompt,
            },
            {
                "role": "user",
                "content": f"ç°åœ¨éœ€è¦å›ç­”è¿™ä¸ªé—®é¢˜{question}å¯ä»¥ä½¿ç”¨çš„å·¥å…·å‡½æ•°æœ‰{filtered_tools}è¯·å…ˆç»“åˆæä¾›çš„å·¥å…·å‡½æ•°æ·±åº¦æ€è€ƒï¼Œè¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨ä»¥åŠè§£ç­”æ€è·¯ï¼Œä¸éœ€è¦åšå‡ºæœ€ç»ˆçš„å›ç­”ã€‚",
            },
        ]

        # ç¬¬ä¸€æ¬¡è°ƒç”¨æ¨¡å‹
        response = glm4_create(6, messages)
        messages.append(response.choices[0].message.model_dump())

        function_results = []
        # æœ€å¤§è¿­ä»£æ¬¡æ•°
        iteration = 1
        while True:
            iteration += 1
            if (
                response.choices[0].message.content
                and "å·²å®Œæˆå›ç­”" in response.choices[0].message.content
            ):
                break

            if response.choices[0].finish_reason == "tool_calls":
                for tool_call in response.choices[0].message.tool_calls:
                    # è·å–å·¥å…·è°ƒç”¨ä¿¡æ¯
                    print("! è°ƒç”¨å‡½æ•°:", tool_call)
                    args = json.loads(tool_call.function.arguments)
                    function_name = tool_call.function.name

                    # æ‰§è¡Œå·¥å…·å‡½æ•°
                    if function_name in function_map:
                        print(f"! æ‰§è¡Œå·¥å…·å‡½æ•°: {function_name}ï¼Œå‚æ•°: {args}")
                        function_result = function_map[function_name](**args)
                        print(f"! å·¥å…·å‡½æ•°æ‰§è¡Œç»“æœ: {function_result}")

                        function_results.append(function_result)
                        messages.append(
                            {
                                "role": "tool",
                                "content": f"{function_result}",
                                "tool_call_id": tool_call.id,
                            }
                        )
                    else:
                        print(f"æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {function_name}")
                        break
                print(f"ç¬¬{iteration}æ¬¡è°ƒç”¨æ¨¡å‹")
                response = glm4_create(8, messages, filtered_tools)
            else:
                messages.append(response.choices[0].message.model_dump())
                messages.append(
                    {
                        "role": "user",
                        "content": "ç°åœ¨è¯·ä½¿ç”¨å‡½æ•°å®Œæˆå›ç­”ã€‚è‹¥å·²å®Œæˆå›ç­”ï¼Œè¯·åªè¾“å‡º'å·²å®Œæˆå›ç­”'ã€‚å¦åˆ™è¯·ç»§ç»­æ¨ç†ï¼Œè‹¥æœ‰éœ€è¦åˆ™è°ƒç”¨å‡½æ•°ã€‚",
                    }
                )
                print(f"ç¬¬{iteration}æ¬¡è°ƒç”¨æ¨¡å‹")
                response = glm4_create(8, messages, filtered_tools)
        messages.append(response.choices[0].message.model_dump())
        messages.append(
            {
                "role": "user",
                "content": "è¯·æ ¹æ®ä¸Šè¿°å›ç­”è¿‡ç¨‹ï¼Œç®€æ´åœ°å›ç­”é—®é¢˜ï¼Œä¸è¦åˆ†æ®µè½ã€‚æ³¨æ„åŠ ä¸Šæ•°å€¼çš„å•ä½ã€å›ç­”æ ¼å¼ç­‰ã€‚",
            }
        )
        print(f"ç¬¬{iteration}æ¬¡è°ƒç”¨æ¨¡å‹")
        response = glm4_create(8, messages, filtered_tools)
        return response.choices[0].message.content, str(function_results)
    except Exception as e:
        print(f"Error generating answer for question: {question}, {e}")
        return None, None

<<<<<<< HEAD
def get_answer_in_subtask_way(question, tools, temple, api_look: bool = True):
=======
def get_answer_2(question, tools, temple, api_look: bool = True):
>>>>>>> origin/subtask-Ver
    filtered_tools = tools
    try:
        messages = [
            {
                "role": "system",
                "content": initial_prompt,
            },
            {"role": "user", "content": question},
        ]

        # **ä»»åŠ¡æ‹†è§£ï¼ˆCoTï¼‰**
        task_decomposition_prompt = f"""
<<<<<<< HEAD
                        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡æ‹†è§£åŠ©æ‰‹ã€‚è¯·é€æ­¥æ€è€ƒï¼Œæ‹†è§£æˆå¤šä¸ªå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ç¡®å®šéœ€è¦è°ƒç”¨çš„ APIã€‚
                        ä½ åªèƒ½ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œä¸¥æ ¼çœ‹ä¸‹å·¥å…·çš„è¾“å…¥ä¸è¾“å‡º{[{tool['function']['name']: tool['function']['description']} for tool in filtered_tools]}ã€‚
                        è¿™æ˜¯ä»»åŠ¡åˆ†è§£çš„å‚è€ƒæ˜¯{[{"question": one_temple["question"], "subtasks": one_temple["subtasks"]} for one_temple in temple]}
                        å¦‚æœéœ€è¦è‡ªæˆ‘æ€è€ƒå¤„ç†ï¼Œå°±åœ¨apié‡Œå¡«å…¥"self_thought"ã€‚
                        å¦‚æœéœ€è¦è¾“å‡ºï¼Œåˆ™apié‡Œå¡«å…¥"out_put"ã€‚
                        åªéœ€è¦ä»»åŠ¡åˆ†è§£çš„JSONï¼Œå°½å¯èƒ½ç»†è‡´ï¼ŒJSONåªéœ€è¦stepï¼Œtaskå’Œapiï¼Œä¸éœ€è¦JSONçš„æ ¼å¼åŒ–æ ‡è®°ã€‚
                        è¿”å› JSONçš„æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                        {{
                            "subtasks": [
                                {{"step": 1, "task": "æŸ¥è¯¢è®¾å¤‡çŠ¶æ€", "api": "get_device_status_by_time_range"}},
                                {{"step": 2, "task": "è®¡ç®—èƒ½è€—", "api": "calculate_total_energy"}}
                            ]
                        }}
                        é—®é¢˜ï¼š{question}
                        """
=======
                    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡æ‹†è§£åŠ©æ‰‹ã€‚è¯·é€æ­¥æ€è€ƒï¼Œæ‹†è§£æˆå¤šä¸ªå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ç¡®å®šéœ€è¦è°ƒç”¨çš„ APIã€‚
                    ä½ åªèƒ½ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œä¸¥æ ¼çœ‹ä¸‹å·¥å…·çš„è¾“å…¥ä¸è¾“å‡º{[{tool['function']['name']: tool['function']['description']} for tool in filtered_tools]}ã€‚
                    è¿™æ˜¯ä»»åŠ¡åˆ†è§£çš„å‚è€ƒæ˜¯{[{"question": one_temple["question"], "subtasks": one_temple["subtasks"]} for one_temple in temple]}
                    å¦‚æœéœ€è¦è‡ªæˆ‘æ€è€ƒå¤„ç†ï¼Œå°±åœ¨apié‡Œå¡«å…¥"self_thought"ã€‚
                    å¦‚æœéœ€è¦è¾“å‡ºï¼Œåˆ™apié‡Œå¡«å…¥"out_put"ã€‚
                    åªéœ€è¦ä»»åŠ¡åˆ†è§£çš„JSONï¼Œå°½å¯èƒ½ç»†è‡´ï¼ŒJSONåªéœ€è¦stepï¼Œtaskå’Œapiï¼Œä¸éœ€è¦JSONçš„æ ¼å¼åŒ–æ ‡è®°ã€‚
                    è¿”å› JSONçš„æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                    {{
                        "subtasks": [
                            {{"step": 1, "task": "æŸ¥è¯¢è®¾å¤‡çŠ¶æ€", "api": "get_device_status_by_time_range"}},
                            {{"step": 2, "task": "è®¡ç®—èƒ½è€—", "api": "calculate_total_energy"}}
                        ]
                    }}
                    é—®é¢˜ï¼š{question}
                    """
>>>>>>> origin/subtask-Ver
        task_response = glm4_create(3, [{"role": "user", "content": task_decomposition_prompt}], [])

        def clean_json_text(text):
            # å»é™¤ Markdown ä»£ç å—åŒ…è£¹ï¼ˆ```json ... ```)
            text = re.sub(r"^```json\s*", "", text)  # å»æ‰å¼€å¤´çš„ ```json
            text = re.sub(r"```$", "", text)  # å»æ‰ç»“å°¾çš„ ```
            return text.strip()  # é¢å¤–å»æ‰å‰åç©ºæ ¼

<<<<<<< HEAD
        clear_text = clean_json_text(task_response.choices[0].message.content)
        print("! ä»»åŠ¡æ‹†è§£ç»“æœ:", clear_text)

        self_thought_memory = {"steps": []}  # è®°å½•æ‰€æœ‰ step çš„è¾“å…¥è¾“å‡º
=======

        clear_text = clean_json_text(task_response.choices[0].message.content)
        print("! ä»»åŠ¡æ‹†è§£ç»“æœ:", clear_text)

        self_thought_memory = {"steps" : []}  # è®°å½•æ‰€æœ‰ step çš„è¾“å…¥è¾“å‡º
>>>>>>> origin/subtask-Ver

        # è§£æä»»åŠ¡æ‹†è§£ç»“æœ
        task_decomposition = json.loads(clear_text)
        subtasks = task_decomposition.get("subtasks", [])

        function_results = []
        messages.append({"role": "assistant", "content": f"ä»»åŠ¡æ‹†è§£: {subtasks}"})

        for subtask in subtasks:
            print(f"æ‰§è¡Œå­ä»»åŠ¡ {subtask['step']}: {subtask['task']}")
            print(self_thought_memory)

            # **è®© LLM ç”Ÿæˆå­ä»»åŠ¡çš„å…·ä½“æ‰§è¡ŒæŒ‡ä»¤**
            subtask_prompt = f"""
<<<<<<< HEAD
                    ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                    {subtask["task"]}
                    ä½ éœ€è¦è°ƒç”¨çš„å·¥å…·æ˜¯:{subtask["api"]}
                    ä½ åªéœ€è¦æä¾›å‚æ•°å°±è¡Œï¼Œä¸è¦æ€è€ƒåˆ«çš„æ–¹æ³•ã€‚
                    è¯·ç»“åˆ CoT æ–¹å¼æ€è€ƒï¼Œé€æ­¥æ‹†è§£æ‰§è¡Œï¼Œå¹¶è°ƒç”¨åˆé€‚çš„å·¥å…·ã€‚
                """

            if subtask['api'] == "self_thought":
                subtask_prompt = f"""
                        ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                        {subtask["task"]}
                        - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                        {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}
                        ä½ éœ€è¦ç»“åˆä¹‹å‰çš„è¾“å…¥è¾“å‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå›ç­”æœ€å¥½ä¸»è°“å®¾ä¸€å¥è¯ä¸€æ°”å‘µæˆã€‚
                    """
            if subtask['api'] == "out_put":
                print("final")
                subtask_prompt = f"""
                        ä½ è¦è§£å†³çš„é—®é¢˜æ˜¯:{question}
                        ä½ åªéœ€è¦é€šè¿‡å†å²è®°å½•å‘Šè¯‰æˆ‘ç­”æ¡ˆï¼Œä¸éœ€è¦è°ƒç”¨ä»»ä½•å·¥å…·ã€‚
                        - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                        {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}ã€‚
                    """
=======
                ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                {subtask["task"]}
                ä½ éœ€è¦è°ƒç”¨çš„å·¥å…·æ˜¯:{subtask["api"]}
                ä½ åªéœ€è¦æä¾›å‚æ•°å°±è¡Œï¼Œä¸è¦æ€è€ƒåˆ«çš„æ–¹æ³•ã€‚
                è¯·ç»“åˆ CoT æ–¹å¼æ€è€ƒï¼Œé€æ­¥æ‹†è§£æ‰§è¡Œï¼Œå¹¶è°ƒç”¨åˆé€‚çš„å·¥å…·ã€‚
            """

            if subtask['api'] == "self_thought":
                subtask_prompt = f"""
                    ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                    {subtask["task"]}
                    - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                    {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}
                    ä½ éœ€è¦ç»“åˆä¹‹å‰çš„è¾“å…¥è¾“å‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå›ç­”æœ€å¥½ä¸»è°“å®¾ä¸€å¥è¯ä¸€æ°”å‘µæˆã€‚
                """
            if subtask['api'] == "out_put":
                print("final")
                subtask_prompt = f"""
                    ä½ è¦è§£å†³çš„é—®é¢˜æ˜¯:{question}
                    ä½ åªéœ€è¦é€šè¿‡å†å²è®°å½•å‘Šè¯‰æˆ‘ç­”æ¡ˆï¼Œä¸éœ€è¦è°ƒç”¨ä»»ä½•å·¥å…·ã€‚
                    - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                    {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}ã€‚
                """
>>>>>>> origin/subtask-Ver
            response = glm4_create(6, [{"role": "user", "content": subtask_prompt}], filtered_tools)
            messages.append(response.choices[0].message.model_dump())

            output = response.choices[0].message.model_dump()

            if not response.choices[0].message.tool_calls:
                continue  # å¦‚æœ LLM æ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œè·³è¿‡

            for tool_call in response.choices[0].message.tool_calls:
                args = json.loads(tool_call.function.arguments)
                function_name = tool_call.function.name

                if function_name in function_map:
                    print(f"! æ‰§è¡Œå·¥å…·å‡½æ•°: {function_name}ï¼Œå‚æ•°: {args}")
                    function_result = function_map[function_name](**args)

                    output = function_result

                    function_results.append(function_result)
                    messages.append({"role": "tool", "content": f"{function_result}", "tool_call_id": tool_call.id})
                else:
                    print(f"æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {function_name}")

                # **å­˜å‚¨ step è®°å½•**
                self_thought_memory["steps"].append({
                    "step": subtask["step"],
                    "input": subtask["task"],
                    "output": output
                })

        # **æœ€ç»ˆæ€»ç»“å›ç­”**
<<<<<<< HEAD
        messages.append(
            {"role": "user", "content": "è¯·æ€»ç»“æ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ä¸€å¥è¯å°½å¯èƒ½è¯¦ç»†å¾—å›ç­”é—®é¢˜ï¼Œä¸ç”¨ç»™æˆ‘è¿‡ç¨‹ã€‚å¦‚æœé¢˜ç›®æœ‰è¾“å‡ºè¦æ±‚ï¼Œä¸¥æ ¼æ‰§è¡Œï¼Œå¦‚æœ‰å•ä½ï¼Œè®°å¾—å•ä½ã€‚"})
=======
        messages.append({"role": "user", "content": "è¯·æ€»ç»“æ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ä¸€å¥è¯å°½å¯èƒ½è¯¦ç»†å¾—å›ç­”é—®é¢˜ï¼Œä¸ç”¨ç»™æˆ‘è¿‡ç¨‹ã€‚å¦‚æœé¢˜ç›®æœ‰è¾“å‡ºè¦æ±‚ï¼Œä¸¥æ ¼æ‰§è¡Œï¼Œå¦‚æœ‰å•ä½ï¼Œè®°å¾—å•ä½ã€‚"})
>>>>>>> origin/subtask-Ver
        final_response = glm4_create(6, messages, filtered_tools)
        return final_response.choices[0].message.content, str(function_results)
    except Exception as e:
        print(f"Error generating answer for question: {question}, {e}")
        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        return None, None


def select_api_based_on_question(question, tools):

    temple = []

<<<<<<< HEAD
    api_list_filter = ["calculate_percent", "query_device_parameter", "sum_two"]
    # æ ¹æ®é—®é¢˜å†…å®¹é€‰æ‹©ç›¸åº”çš„ API
    if "èƒ½è€—" in question or "åšåŠŸ" in question:
        print("! é—®é¢˜åŒ…å«ï¼šèƒ½è€—")
        if "æ¨è¿›" in question or "ä¾§æ¨" in question:
            print("! é—®é¢˜åŒ…å«ï¼šæ¨è¿›ï¼Œæä¾›Apiï¼šcalculate_total_energy_consumption")
=======
    api_list_filter = ["calculate_percent", "query_device_parameter", "sum_list"]
    # æ ¹æ®é—®é¢˜å†…å®¹é€‰æ‹©ç›¸åº”çš„ API
    if "èƒ½è€—" in question:
        print("! é—®é¢˜åŒ…å«ï¼šèƒ½è€—ï¼Œæä¾›Apiï¼šcalculate_total_energy")
        api_list_filter.append("calculate_total_energy")
        if "æ¨è¿›" in question or "ä¾§æ¨" in question:
            print("! é—®é¢˜åŒ…å«ï¼šæ¨è¿›ç³»ç»Ÿï¼Œæä¾›Apiï¼šcalculate_total_energy_consumption")
>>>>>>> origin/subtask-Ver
            api_list_filter.append("calculate_total_energy_consumption")
        if "ç”²æ¿æœºæ¢°" in question or "æŠ˜è‡‚åŠè½¦" in question or "Aæ¶" in question:
            print(
                "! é—®é¢˜åŒ…å«ï¼šç”²æ¿æœºæ¢°è®¾å¤‡ï¼Œæä¾›Apiï¼šcalculate_total_deck_machinery_energy"
            )
            api_list_filter.append("calculate_total_deck_machinery_energy")
        if "å‘ç”µæœº" in question:
            print("! é—®é¢˜åŒ…å«ï¼šå‘ç”µæœºï¼Œæä¾›Apiï¼šcalculate_generator_energy_consumption")
            api_list_filter.append("calculate_generator_energy_consumption")
        if "èˆµæ¡¨" in question:
            print("! é—®é¢˜åŒ…å«ï¼šèˆµæ¡¨ï¼Œæä¾›Apiï¼šcalculate_total_rudder_energy")
            api_list_filter.append("calculate_total_rudder_energy")
    if (
        "åŠ¨ä½œ" in question
        or "DP" in question
        or "æ‘†" in question
        or "å¼€æœº" in question
        or "å°è‰‡" in question
        or "å¾æœè€…" in question
        or "å…³æœº" in question
    ):
        print("! é—®é¢˜åŒ…å«ï¼šåŠ¨ä½œï¼Œæä¾›Apiï¼šget_device_status_by_time_range")
        api_list_filter.append("get_device_status_by_time_range")
    if "å¼€æœºæ—¶é•¿" in question or "å¼€æœºæ€»æ—¶é•¿" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå¼€æœºæ—¶é•¿ï¼Œä¾›Apiï¼šcalculate_uptime")
        api_list_filter.append("calculate_uptime")
    if (
        "è¿è¡Œæ—¶é•¿" in question or "è¿è¡Œæ—¶é—´" in question
    ) and "å®é™…è¿è¡Œæ—¶é•¿" not in question:
        print("! é—®é¢˜åŒ…å«ï¼šè¿è¡Œæ—¶é•¿ï¼Œä¸åŒ…å«ï¼šå®é™…è¿è¡Œæ—¶é•¿ï¼Œæä¾›Apiï¼šcalculate_uptime")
        api_list_filter.append("calculate_uptime")
    if "Aæ¶" in question and "å¼‚å¸¸" in question:
        print("! é—®é¢˜åŒ…å«ï¼šAæ¶ã€å¼‚å¸¸ï¼Œæä¾›Apiï¼šcheck_ajia_angle")
        api_list_filter.append("check_ajia_angle")
    if "ç‡ƒæ²¹æ¶ˆè€—é‡" in question:
        print(
            "! é—®é¢˜åŒ…å«ï¼šç‡ƒæ²¹æ¶ˆè€—é‡ï¼Œæä¾›Apiï¼šcalculate_fuel_consumption, calculate_fuel_consumption_weight"
        )
        api_list_filter.append("calculate_fuel_consumption")
        api_list_filter.append("calculate_fuel_consumption_weight")
    if "å‘ç”µé‡" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå‘ç”µé‡ï¼Œæä¾›Apiï¼šcalculate_generator_energy_consumption")
        api_list_filter.append("calculate_generator_energy_consumption")
    if "ç†è®ºå‘ç”µé‡" in question:
        print("! é—®é¢˜åŒ…å«ï¼šç†è®ºå‘ç”µé‡ï¼Œæä¾›Apiï¼šcalculate_theoretical_energy_output")
        api_list_filter.append("calculate_theoretical_energy_output")
    if "å­—æ®µåç§°" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå­—æ®µåç§°ï¼Œæä¾›Apiï¼šget_field_dict")
        api_list_filter.append("get_field_dict")
    if "å®é™…è¿è¡Œæ—¶é•¿" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå®é™…è¿è¡Œæ—¶é•¿ï¼Œæä¾›Apiï¼šcompute_operational_duration")
        api_list_filter.append("compute_operational_duration")
    if "ä½œä¸š" in question:
        print("! é—®é¢˜åŒ…å«ï¼šä½œä¸šï¼Œæä¾›Apiï¼šget_work_time")
        api_list_filter.append("get_work_time")
    if "æ•°æ®" in question and "ç¼ºå¤±" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæ•°æ®ã€ç¼ºå¤±ï¼Œæä¾›Apiï¼šfind_missing_records")
        temple.append(Sample.task_temple["æ•°æ®ç¼ºå¤±"])
        api_list_filter.append("find_missing_records")
<<<<<<< HEAD
        temple.append(Sample.task_temple["æ•°æ®ç¼ºå¤±"])
=======
        if "ç¼ºå¤±æ¯”ä¾‹" in question:
            api_list_filter.remove("calculate_percent")
>>>>>>> origin/subtask-Ver
    if "æ‘†åŠ¨" in question and "æ¬¡æ•°" in question:
        print(
            "! é—®é¢˜åŒ…å«ï¼šæ‘†åŠ¨ã€æ¬¡æ•°ï¼Œæä¾›Apiï¼šcount_swing_with_ruleã€count_swing_with_threshold"
        )
        api_list_filter.append("count_swing_with_rule")
        api_list_filter.append("count_swing_with_threshold")
    if "æœ€å°å€¼" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæœ€å°å€¼ï¼Œæä¾›Apiï¼šfind_min_value")
        api_list_filter.append("find_min_value")
    if "æœ€å¤§å€¼" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæœ€å¤§å€¼ï¼Œæä¾›Apiï¼šfind_max_value")
        api_list_filter.append("find_max_value")
    if "å¹³å‡å€¼" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå¹³å‡å€¼ï¼Œæä¾›Apiï¼šfind_avg_value")
        api_list_filter.append("find_avg_value")
    if "æ—¶é—´å·®" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæ—¶é—´å·®ï¼Œæä¾›Apiï¼šcalculate_time_difference")
        api_list_filter.append("calculate_time_difference")

    if len(api_list_filter) == 3:
        # å¦‚æœé—®é¢˜ä¸åŒ¹é…ä¸Šè¿°æ¡ä»¶ï¼Œåˆ™æ ¹æ®è¡¨åé€‰æ‹© API
        table_name_string = choose_table(question)
        with open("dict.json", "r", encoding="utf-8") as file:
            table_data = json.load(file)
            table_name = [
                item for item in table_data if item["æ•°æ®è¡¨å"] in table_name_string
            ]
            if "è®¾å¤‡å‚æ•°è¯¦æƒ…è¡¨" in [item["æ•°æ®è¡¨å"] for item in table_name]:
                print("! ä½¿ç”¨è®¾å¤‡å‚æ•°è¯¦æƒ…è¡¨ï¼Œæä¾›Apiï¼šquery_device_parameter")
                api_list_filter.append("query_device_parameter")
                content_p_1 = str(table_name) + question  # è¡¥å…… content_p_1
            else:
                print("! ä½¿ç”¨æ•°æ®è¡¨ï¼Œæä¾›Apiï¼šget_table_data")
                api_list_filter.append("get_table_data")
                content_p_1 = str(table_name) + question

    # è¿‡æ»¤å·¥å…·åˆ—è¡¨
    filtered_tools = [
        tool
        for tool in tools
        if tool.get("function", {}).get("name") in api_list_filter
    ]
    # è¿”å›ç»“æœ
    if "content_p_1" in locals():
        return content_p_1, filtered_tools, temple
    else:
        return question, filtered_tools, temple


def enhanced(prompt: str, context=None, instructions=None, modifiers=None):
    """
    å¢å¼ºæç¤ºè¯å‡½æ•°
    """
    enhanced_prompt = prompt
    enhanced_prompt = enhanced_prompt.replace(
        "XXå°æ—¶XXåˆ†é’Ÿ", "XXå°æ—¶XXåˆ†é’Ÿï¼Œ01å°æ—¶01åˆ†é’Ÿæ ¼å¼"
    )
    enhanced_prompt = enhanced_prompt.replace(
        "å‘ç”Ÿäº†ä»€ä¹ˆ", "ä»€ä¹ˆè®¾å¤‡åœ¨è¿›è¡Œä»€ä¹ˆåŠ¨ä½œï¼ŒåŠ¨ä½œç›´æ¥å¼•ç”¨ä¸è¦ä¿®æ”¹,å¦‚ã€Aæ¶æ‘†å›ã€‘"
    )
    enhanced_prompt = enhanced_prompt.replace(
        "ä»€ä¹ˆè®¾å¤‡è¿›è¡Œäº†ä»€ä¹ˆå…³é”®åŠ¨ä½œ",
        "ä»€ä¹ˆè®¾å¤‡è¿›è¡Œäº†ä»€ä¹ˆå…³é”®åŠ¨ä½œï¼ˆåˆ—ä¸¾æ‰€æœ‰è¿›è¡Œäº†æŸä¸ªåŠ¨ä½œçš„è®¾å¤‡ï¼‰",
    )
    enhanced_prompt = enhanced_prompt.replace(
        "è¿è¡Œçš„å¹³å‡æ—¶é—´", "è¿è¡Œçš„å¹³å‡æ—¶é—´ï¼ˆæ¯å¤©è¿è¡Œæ—¶é•¿çš„å¹³å‡å€¼ï¼‰"
    )
    if "ä½œä¸š" in enhanced_prompt and "å¼€å§‹" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "è‹¥é—®é¢˜æ²¡æœ‰ç‰¹æ®Šè¯´æ˜ï¼Œåˆ™æ·±æµ·ä½œä¸šAçš„å¼€å§‹ä»¥ON_DPä¸ºæ ‡å¿—ï¼ŒDPåŠ¨ä½œæ¥è‡ªå®šä½è®¾å¤‡ã€‚"
        )
    if "Aæ¶" in enhanced_prompt and "å¼€å¯" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆAæ¶çš„å¼€å¯æ—¶é—´ä»¥Aæ¶å¼€æœºè¿™ä¸ªåŠ¨ä½œå‘ç”Ÿçš„æ—¶é—´ä¸ºå‡†ï¼Œå…¶ä»–åŠ¨ä½œå‘ç”Ÿçš„æ—¶é—´ä¸ç®—ã€‚ï¼‰"
        )
    if "Aæ¶å¼€æœº" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆAæ¶å¼€æœºå°±æ˜¯â€œAæ¶â€è¿™ä¸ªè®¾å¤‡å‘ç”Ÿâ€œå¼€æœºâ€è¿™ä¸ªåŠ¨ä½œã€‚ï¼‰"
        )
    if "å‘ç”µæœºç»„" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆè‹¥é—®é¢˜åªè¯´å‘ç”µæœºç»„ï¼Œæœªè¯´æ˜æ˜¯å“ªä¸ªå‘ç”µæœºç»„ï¼Œåˆ™è®¤ä¸ºæ˜¯æ‰€æœ‰å››ä¸ªå‘ç”µæœºç»„çš„æ€»ä½“å€¼ã€‚ï¼‰"
        )
    if "ä½œä¸šèƒ½è€—" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆä½œä¸šèƒ½è€—æ˜¯æŒ‡æ·±æµ·ä½œä¸šAä¸­æ‰€æœ‰å‘ç”µæœºçš„èƒ½è€—ï¼Œåº”å…ˆç”¨get_work_timeåˆ—å‡ºæ‰€æœ‰ä½œä¸šçš„æ—¶é—´èŒƒå›´ï¼Œå†ç”¨calculate_generator_energy_consumptionè®¡ç®—æ¯ä¸ªä½œä¸šæ—¶é—´èŒƒå›´çš„å‘ç”µæœºèƒ½è€—ï¼Œæœ€åç”¨sum_twoè®¡ç®—æ€»å’Œã€‚ï¼‰"
        )
    if "å‘ç”µæ•ˆç‡" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆå‘ç”µæ•ˆç‡æ˜¯æŒ‡å‘ç”µæœºç»„èƒ½è€—ä¸ç†è®ºå‘ç”µé‡çš„æ¯”å€¼ã€‚ï¼‰"
        )
    if "ç†è®ºå‘ç”µé‡" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆè¦è®¡ç®—ç†è®ºå‘ç”µé‡åº”å…ˆè®¡ç®—ç‡ƒæ²¹æ¶ˆè€—é‡ã€‚ï¼‰"
    if "å®é™…è¿è¡Œæ—¶é•¿" in enhanced_prompt and "æ•ˆç‡" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆæ•ˆç‡æ˜¯æŒ‡å®é™…è¿è¡Œæ—¶é•¿å’Œå¼€æœºæ—¶é•¿çš„æ¯”å€¼ã€‚ï¼‰"
    if "DPè¿‡ç¨‹" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆDPè¿‡ç¨‹æ˜¯æŒ‡ä»ON_DPåˆ°OFF_DPçš„è¿‡ç¨‹ï¼Œåº”å…ˆä½¿ç”¨get_device_status_by_time_rangeæŸ¥è¯¢æ‰€æœ‰ON_DPå’ŒOFF_DPçš„æ—¶é—´ï¼Œå†è®¡ç®—æ‰€æœ‰DPè¿‡ç¨‹çš„èƒ½è€—ï¼Œæœ€åä½¿ç”¨sum_twoå‡½æ•°å–æ€»å’Œã€‚ï¼‰"
        )
    if "å°è‰‡å…¥æ°´åˆ°å°è‰‡è½åº§" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆå°è‰‡å…¥æ°´åˆ°å°è‰‡è½åº§æ˜¯æŒ‡å°è‰‡å…¥æ°´åŠ¨ä½œå‘ç”Ÿåˆ°å°è‰‡è½åº§åŠ¨ä½œå‘ç”Ÿçš„æ•´ä¸ªæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨å‡½æ•°è®¡ç®—è¿™æ®µæ—¶é—´å†…çš„èƒ½è€—æ—¶åº”è¯¥ä¼ å…¥è¿™ä¸ªæ—¶é—´èŒƒå›´ã€‚ï¼‰"
        )
    if "æ•°æ®" in enhanced_prompt and "ç¼ºå¤±" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆé—®é¢˜ä¸­ç»™å‡ºçš„æ•°æ®è¡¨åç§°å¯èƒ½æœ‰è¯¯ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¹‹å‰ç»™å‡ºçš„æ•°æ®è¡¨åæ¥æŸ¥è¯¢ã€‚ï¼‰"
        )
    if "ä»ON DPåˆ°OFF DPæœŸé—´" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆåº”è€ƒè™‘æ‰€æœ‰ON_DPåˆ°OFF_DPçš„èŒƒå›´ã€‚ï¼‰"
    if "1~4å·" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆè‹¥é—®é¢˜æ²¡ç‰¹åˆ«è¯´æ˜ï¼Œåˆ™å–æ€»å€¼ï¼‰"
    if "Aæ¶çš„æ€»èƒ½è€—" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆAæ¶çš„æ€»èƒ½è€—æ˜¯æŒ‡ä¸€å·é—¨æ¶å’ŒäºŒå·é—¨æ¶çš„èƒ½è€—æ€»å’Œã€‚ï¼‰"
        )
    if "å›æ”¶" in enhanced_prompt and "å¸ƒæ”¾" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆæ·±æµ·ä½œä¸šåˆ†ä¸ºä¸‹æ”¾ï¼ˆå¸ƒæ”¾ï¼‰é˜¶æ®µå’Œå›æ”¶é˜¶æ®µï¼Œä¸€èˆ¬ä¸‹æ”¾é˜¶æ®µå‘ç”Ÿåœ¨å›æ”¶é˜¶æ®µä¹‹å‰ã€‚ï¼‰"
        )
    if "æŠ¥è­¦" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆç¬¦å·'â†‘'è¡¨ç¤ºåªæœ‰åœ¨æ•°æ®å¤§äºæŠ¥è­¦å€¼æ—¶æ‰æŠ¥è­¦ï¼Œç¬¦å·'â†“'è¡¨ç¤ºå½“æ•°æ®å°äºæŠ¥è­¦å€¼æ—¶æŠ¥è­¦ã€‚ä¾‹å¦‚ï¼Œé¢˜ç›®è¯´å‚æ•°ä¸ºè¶…è¿‡160ï¼ŒæŠ¥è­¦å€¼ä¸º730â†‘ï¼Œåˆ™å¯èƒ½ä¸æŠ¥è­¦ï¼Œå› ä¸ºå¯èƒ½æ˜¯å°äº730ã€‚ä¾‹å¦‚ï¼Œé¢˜ç›®è¯´å‚æ•°ä¸ºä½äº500ï¼ŒæŠ¥è­¦å€¼ä¸º210â†“ï¼Œåˆ™å¯èƒ½ä¸æŠ¥è­¦ï¼Œå› ä¸ºå¯èƒ½æ˜¯å¤§äº210ã€‚ï¼‰"
        )
    if "å¼€æœºæ—¶é•¿" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆè‹¥é—®é¢˜æ²¡æœ‰å¦å¤–è¯´æ˜ï¼Œå¼€æœºæ—¶é•¿ä»¥åˆ†é’Ÿä¸ºå•ä½ã€‚ï¼‰"
        )
    if "æ—¶é—´å·®" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆè®¡ç®—æ—¶é—´å·®å‰å…ˆè¯´æ˜å‘ç”Ÿçš„åŠ¨ä½œä»¥åŠè¦ä½¿ç”¨çš„æ—¶é—´ï¼Œä½¿ç”¨å·¥å…·å‡½æ•°è®¡ç®—æ—¶é—´å·®æ—¶æ³¨æ„è¾“å…¥çš„ä¸¤ä¸ªæ—¶é—´çš„å…ˆåã€‚ï¼‰"
        )

    print("! å¢å¼ºæç¤ºè¯ï¼š", enhanced_prompt)
    return enhanced_prompt


def run_conversation_xietong(question):
    question = enhanced(question)
    content_p_1, filtered_tool, temple = select_api_based_on_question(question, tools.tools_all)
    print("content_p_1:", content_p_1)
    print("filtered_tool:", [tool["function"]["name"] for tool in filtered_tool])
    answer, select_result = get_answer_2(
        question=content_p_1,
        tools=filtered_tool,
        temple=temple,
        api_look=False
    )
    return answer


def get_answer_normal_way(question):
    try:
        print(f"! å°è¯•è§£å†³é—®é¢˜ï¼š{question}")
        last_answer = run_conversation_xietong(question)
<<<<<<< HEAD
        return last_answer
    except Exception as e:
        print(f"Error occurred while executing get_answer: {e}")
        return "An error occurred while retrieving the answer."

def get_answer_subtask_way(question):
    try:
        print(f"! å°è¯•è§£å†³é—®é¢˜ï¼š{question}")

        def run_conversation_xietong(question):
            question = enhanced(question)
            content_p_1, filtered_tool, temple = select_api_based_on_question(question, tools.tools_all)
            print("content_p_1:", content_p_1)
            print("filtered_tool:", [tool["function"]["name"] for tool in filtered_tool])
            answer, select_result = get_answer_in_subtask_way(
                question=content_p_1,
                tools=filtered_tool,
                temple=temple,
                api_look=False
            )
            return answer

        last_answer = run_conversation_xietong(question)
=======
        # last_answer = last_answer.replace(" ", "")
>>>>>>> origin/subtask-Ver
        return last_answer
    except Exception as e:
        traceback.print_exc()
        print(f"Error occurred while executing get_answer: {e}")
        return "An error occurred while retrieving the answer."


if __name__ == "__main__":
<<<<<<< HEAD
    # é—®é¢˜ç¼–å·
    QUESTION = 100

    with open("NexAI_result.jsonl", "r", encoding="utf-8") as file:
=======
    QUESTION = 100 # é—®é¢˜ç¼–å·
    with open("result.jsonl", "r", encoding="utf-8") as file:
>>>>>>> origin/subtask-Ver
        question_list = [json.loads(line.strip()) for line in file]
    question = question_list[QUESTION - 1]["question"]
    answer = get_answer_subtask_way(question)
    # answer = get_answer_normal_way(question)
    while not answer:
        answer = get_answer_normal_way(question)

    print("*******************æœ€ç»ˆç­”æ¡ˆ***********************")
    print(answer)

    question_list[QUESTION - 1]["answer"] = answer

    with open("NexAI_result.jsonl", "w", encoding="utf-8") as file:
        for item in question_list:
            file.write(json.dumps(item, ensure_ascii=False) + "\n")
