import json
import os
import traceback
import re

import json

from zhipuai import ZhipuAI

import api
import tools
from initial_prompt import initial_prompt
import Sample

folders = ["database_in_use", "data"]
if any(not os.path.exists(folder) for folder in folders):
    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    import data_process  # for data process using
else:
    print("æ‰€æœ‰æ–‡ä»¶å¤¹å‡å·²å­˜åœ¨ã€‚ä¸å†é‡æ–°é¢„å¤„ç†æ•°æ®ã€‚")
    print("éœ€è¦é¢„å¤„ç†æ•°æ®ï¼Œè¯·åˆ é™¤æ–‡ä»¶å¤¹åé‡æ–°è¿è¡Œã€‚")


def create_chat_completion(messages, model="glm-4-plus"):
    print("å‘èµ·AIå¯¹è¯")
    client = ZhipuAI(api_key="6cf617672cae4afa9a280657a87beccb.m5ii3yJ1p3E42abg")
    response = client.chat.completions.create(
        model=model, stream=False, messages=messages
    )
    print("! AIå›å¤:", response.choices[0].message.content)
    return response


def choose_table(question):
    print("å‘AIæŸ¥è¯¢éœ€è¦çš„æ•°æ®è¡¨")
    with open("dict.json", "r", encoding="utf-8") as file:
        context_text = str(json.load(file))
    prompt = f"""æˆ‘æœ‰å¦‚ä¸‹æ•°æ®è¡¨ï¼š<{context_text}>
    ç°åœ¨åŸºäºæ•°æ®è¡¨å›ç­”é—®é¢˜ï¼š{question}ã€‚
    åˆ†æéœ€è¦å“ªäº›æ•°æ®è¡¨ã€‚
    ä»…è¿”å›éœ€è¦çš„æ•°æ®è¡¨åï¼Œæ— éœ€å±•ç¤ºåˆ†æè¿‡ç¨‹ã€‚
    è‹¥é—®é¢˜ä¸­æåˆ°Aæ¶åŠ¨ä½œ,åŒ…æ‹¬å…³æœºã€å¼€æœºã€Aæ¶æ‘†å‡ºã€ç¼†ç»³æŒ‚å¦¥ã€å¾æœè€…å‡ºæ°´ã€å¾æœè€…è½åº§ã€å¾æœè€…èµ·åŠã€å¾æœè€…å…¥æ°´ã€ç¼†ç»³è§£é™¤ã€Aæ¶æ‘†å›ï¼Œåˆ™ä½¿ç”¨Ajia_plc_1è¿™ä¸ªæ•°æ®è¡¨ã€‚
    è‹¥é—®é¢˜ä¸­æåˆ°æŠ˜è‡‚åŠè½¦åŠå°è‰‡åŠ¨ä½œ,åŒ…æ‹¬æŠ˜è‡‚åŠè½¦å…³æœºã€æŠ˜è‡‚åŠè½¦å¼€æœºã€å°è‰‡æ£€æŸ¥å®Œæ¯•ã€å°è‰‡å…¥æ°´ã€å°è‰‡è½åº§ï¼Œåˆ™ä½¿ç”¨device_13_11_meter_1311è¿™ä¸ªæ•°æ®è¡¨ã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    response = create_chat_completion(messages)
    return str(response.choices[0].message.content)


def glm4_create(max_attempts, messages, tools, model="glm-4-plus"):
    print("å‘èµ·AIå¯¹è¯")
    client = ZhipuAI(api_key="6cf617672cae4afa9a280657a87beccb.m5ii3yJ1p3E42abg")
    for attempt in range(max_attempts):
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
        )
        print("! å°è¯•æ¬¡æ•°ï¼š", attempt)
        print("! AIå›å¤:", response.choices[0].message)
        if (
            response.choices
            and response.choices[0].message
            and response.choices[0].message.content
        ):
            if "```python" in response.choices[0].message.content:
                # å¦‚æœç»“æœåŒ…å«å­—ç¬¦ä¸²'python'ï¼Œåˆ™ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
                continue
            else:
                # ä¸€æ—¦ç»“æœä¸åŒ…å«å­—ç¬¦ä¸²'python'ï¼Œåˆ™åœæ­¢å°è¯•
                break
        else:
            return response
    return response


function_map = {
    "calculate_uptime": api.calculate_uptime,
    "compute_operational_duration": api.compute_operational_duration,
    "get_table_data": api.get_table_data,
    "load_and_filter_data": api.load_and_filter_data,
    "calculate_total_energy": api.calculate_total_energy,
    "calculate_total_deck_machinery_energy": api.calculate_total_deck_machinery_energy,
    "query_device_parameter": api.query_device_parameter,
    "get_device_status_by_time_range": api.get_device_status_by_time_range,
    "calculate_total_energy_consumption": api.calculate_total_energy_consumption,
    "calculate_generator_energy_consumption": api.calculate_generator_energy_consumption,
    "check_ajia_angle": api.check_ajia_angle,
    "calculate_fuel_consumption": api.calculate_fuel_consumption,
    "calculate_percent": api.calculate_percent,
    "calculate_theoretical_energy_output": api.calculate_theoretical_energy_output,
    "get_field_dict": api.get_field_dict,
    "sum_list": api.sum_list,
    "get_work_time": api.get_work_time,
    "find_missing_records": api.find_missing_records,
    "count_oscillations": api.count_oscillations,
}


def get_answer_3(question, tools, api_look: bool = True):
    filtered_tools = tools

    print(f"ğŸ”¹ ä»»åŠ¡æ‹†è§£å‰çš„é—®é¢˜ï¼š{question}")

    # **ä»»åŠ¡æ‹†è§£ï¼ˆCoTï¼‰**
    task_decomposition_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡æ‹†è§£åŠ©æ‰‹ã€‚è¯·é€æ­¥æ€è€ƒï¼Œæ‹†è§£æˆå¤šä¸ªå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ç¡®å®šéœ€è¦è°ƒç”¨çš„ APIã€‚
            å…ˆå¯ä»¥ç”¨ä½¿ç”¨çš„å·¥å…·æ—¶{filtered_tools}ã€‚
            åªéœ€è¦ä»»åŠ¡åˆ†è§£çš„JSONï¼Œä¸éœ€è¦å…¶ä»–å†…å®¹ã€‚
            è¿”å› JSON æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            {{
                "subtasks": [
                    {{"step": 1, "task": "æŸ¥è¯¢è®¾å¤‡çŠ¶æ€", "api": "get_device_status_by_time_range"}},
                    {{"step": 2, "task": "è®¡ç®—èƒ½è€—", "api": "calculate_total_energy"}}
                ]
            }}
            é—®é¢˜ï¼š{question}
            """
    print()
    task_response = glm4_create(3, [{"role": "user", "content": task_decomposition_prompt}], [])
    print("! ä»»åŠ¡æ‹†è§£ç»“æœ:", task_response.choices[0].message.content)

    try:
        messages = [
            {
                "role": "system",
                "content": initial_prompt,
            },
            {"role": "user", "content": question},
        ]

        # ç¬¬ä¸€æ¬¡è°ƒç”¨æ¨¡å‹
        response = glm4_create(6, messages, filtered_tools)
        messages.append(response.choices[0].message.model_dump())

        function_results = []
        # æœ€å¤§è¿­ä»£æ¬¡æ•°
        max_iterations = 6
        for _ in range(max_iterations):
            if not response.choices[0].message.tool_calls:
                break

            for tool_call in response.choices[0].message.tool_calls:
                # è·å–å·¥å…·è°ƒç”¨ä¿¡æ¯
                print("! è°ƒç”¨å‡½æ•°:", tool_call)
                args = json.loads(tool_call.function.arguments)
                function_name = tool_call.function.name

                # æ‰§è¡Œå·¥å…·å‡½æ•°
                if function_name in function_map:
                    print(f"! æ‰§è¡Œå·¥å…·å‡½æ•°: {function_name}ï¼Œå‚æ•°: {args}")
                    function_result = function_map[function_name](**args)
                    print(f"! å·¥å…·å‡½æ•°æ‰§è¡Œç»“æœ: {function_result}")

                    function_results.append(function_result)
                    messages.append(
                        {
                            "role": "tool",
                            "content": f"{function_result}",
                            "tool_call_id": tool_call.id,
                        }
                    )
                else:
                    print(f"æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {function_name}")
                    break
            response = glm4_create(8, messages, filtered_tools)
        messages.append(response.choices[0].message.model_dump())
        messages.append(
            {
                "role": "user",
                "content": "è¯·æ ¹æ®ä¸Šè¿°å›ç­”è¿‡ç¨‹ï¼Œç®€æ´åœ°å›ç­”é—®é¢˜ï¼Œä¸è¦åˆ†æ®µè½ã€‚",
            }
        )
        response = glm4_create(8, messages, filtered_tools)
        return response.choices[0].message.content, str(function_results)
    except Exception as e:
        print(f"Error generating answer for question: {question}, {e}")
        return None, None

def get_answer_2(question, tools, temple, api_look: bool = True):
    filtered_tools = tools
    try:
        messages = [
            {
                "role": "system",
                "content": initial_prompt,
            },
            {"role": "user", "content": question},
        ]

        # **ä»»åŠ¡æ‹†è§£ï¼ˆCoTï¼‰**
        task_decomposition_prompt = f"""
                    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡æ‹†è§£åŠ©æ‰‹ã€‚è¯·é€æ­¥æ€è€ƒï¼Œæ‹†è§£æˆå¤šä¸ªå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ç¡®å®šéœ€è¦è°ƒç”¨çš„ APIã€‚
                    ä½ åªèƒ½ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œä¸¥æ ¼çœ‹ä¸‹å·¥å…·çš„è¾“å…¥ä¸è¾“å‡º{[{tool['function']['name']: tool['function']['description']} for tool in filtered_tools]}ã€‚
                    è¿™æ˜¯ä»»åŠ¡åˆ†è§£çš„å‚è€ƒæ˜¯{[{"question": one_temple["question"], "subtasks": one_temple["subtasks"]} for one_temple in temple]}
                    å¦‚æœéœ€è¦è‡ªæˆ‘æ€è€ƒå¤„ç†ï¼Œå°±åœ¨apié‡Œå¡«å…¥"self_thought"ã€‚
                    å¦‚æœéœ€è¦è¾“å‡ºï¼Œåˆ™apié‡Œå¡«å…¥"out_put"ã€‚
                    åªéœ€è¦ä»»åŠ¡åˆ†è§£çš„JSONï¼Œå°½å¯èƒ½ç»†è‡´ï¼ŒJSONåªéœ€è¦stepï¼Œtaskå’Œapiï¼Œä¸éœ€è¦JSONçš„æ ¼å¼åŒ–æ ‡è®°ã€‚
                    è¿”å› JSONçš„æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                    {{
                        "subtasks": [
                            {{"step": 1, "task": "æŸ¥è¯¢è®¾å¤‡çŠ¶æ€", "api": "get_device_status_by_time_range"}},
                            {{"step": 2, "task": "è®¡ç®—èƒ½è€—", "api": "calculate_total_energy"}}
                        ]
                    }}
                    é—®é¢˜ï¼š{question}
                    """
        task_response = glm4_create(3, [{"role": "user", "content": task_decomposition_prompt}], [])

        def clean_json_text(text):
            # å»é™¤ Markdown ä»£ç å—åŒ…è£¹ï¼ˆ```json ... ```)
            text = re.sub(r"^```json\s*", "", text)  # å»æ‰å¼€å¤´çš„ ```json
            text = re.sub(r"```$", "", text)  # å»æ‰ç»“å°¾çš„ ```
            return text.strip()  # é¢å¤–å»æ‰å‰åç©ºæ ¼


        clear_text = clean_json_text(task_response.choices[0].message.content)
        print("! ä»»åŠ¡æ‹†è§£ç»“æœ:", clear_text)

        self_thought_memory = {"steps" : []}  # è®°å½•æ‰€æœ‰ step çš„è¾“å…¥è¾“å‡º

        # è§£æä»»åŠ¡æ‹†è§£ç»“æœ
        task_decomposition = json.loads(clear_text)
        subtasks = task_decomposition.get("subtasks", [])

        function_results = []
        messages.append({"role": "assistant", "content": f"ä»»åŠ¡æ‹†è§£: {subtasks}"})

        for subtask in subtasks:
            print(f"æ‰§è¡Œå­ä»»åŠ¡ {subtask['step']}: {subtask['task']}")
            print(self_thought_memory)

            # **è®© LLM ç”Ÿæˆå­ä»»åŠ¡çš„å…·ä½“æ‰§è¡ŒæŒ‡ä»¤**
            subtask_prompt = f"""
                ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                {subtask["task"]}
                ä½ éœ€è¦è°ƒç”¨çš„å·¥å…·æ˜¯:{subtask["api"]}
                ä½ åªéœ€è¦æä¾›å‚æ•°å°±è¡Œï¼Œä¸è¦æ€è€ƒåˆ«çš„æ–¹æ³•ã€‚
                è¯·ç»“åˆ CoT æ–¹å¼æ€è€ƒï¼Œé€æ­¥æ‹†è§£æ‰§è¡Œï¼Œå¹¶è°ƒç”¨åˆé€‚çš„å·¥å…·ã€‚
            """

            if subtask['api'] == "self_thought":
                subtask_prompt = f"""
                    ä½ éœ€è¦æ‰§è¡Œå¦‚ä¸‹å­ä»»åŠ¡ï¼š
                    {subtask["task"]}
                    - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                    {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}
                    ä½ éœ€è¦ç»“åˆä¹‹å‰çš„è¾“å…¥è¾“å‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå›ç­”æœ€å¥½ä¸»è°“å®¾ä¸€å¥è¯ä¸€æ°”å‘µæˆã€‚
                """
            if subtask['api'] == "out_put":
                print("final")
                subtask_prompt = f"""
                    ä½ è¦è§£å†³çš„é—®é¢˜æ˜¯:{question}
                    ä½ åªéœ€è¦é€šè¿‡å†å²è®°å½•å‘Šè¯‰æˆ‘ç­”æ¡ˆï¼Œä¸éœ€è¦è°ƒç”¨ä»»ä½•å·¥å…·ã€‚
                    - ä½ è¿‡å»çš„ä»»åŠ¡å†å²ï¼ˆè¾“å…¥ & è¾“å‡ºï¼‰å¦‚ä¸‹ï¼š
                    {json.dumps(self_thought_memory, ensure_ascii=False, indent=2, default=str)}ã€‚
                """
            response = glm4_create(6, [{"role": "user", "content": subtask_prompt}], filtered_tools)
            messages.append(response.choices[0].message.model_dump())

            output = response.choices[0].message.model_dump()

            if not response.choices[0].message.tool_calls:
                continue  # å¦‚æœ LLM æ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œè·³è¿‡

            for tool_call in response.choices[0].message.tool_calls:
                args = json.loads(tool_call.function.arguments)
                function_name = tool_call.function.name

                if function_name in function_map:
                    print(f"! æ‰§è¡Œå·¥å…·å‡½æ•°: {function_name}ï¼Œå‚æ•°: {args}")
                    function_result = function_map[function_name](**args)

                    output = function_result

                    function_results.append(function_result)
                    messages.append({"role": "tool", "content": f"{function_result}", "tool_call_id": tool_call.id})
                else:
                    print(f"æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {function_name}")

                # **å­˜å‚¨ step è®°å½•**
                self_thought_memory["steps"].append({
                    "step": subtask["step"],
                    "input": subtask["task"],
                    "output": output
                })

        # **æœ€ç»ˆæ€»ç»“å›ç­”**
        messages.append({"role": "user", "content": "è¯·æ€»ç»“æ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ä¸€å¥è¯å°½å¯èƒ½è¯¦ç»†å¾—å›ç­”é—®é¢˜ï¼Œä¸ç”¨ç»™æˆ‘è¿‡ç¨‹ã€‚å¦‚æœé¢˜ç›®æœ‰è¾“å‡ºè¦æ±‚ï¼Œä¸¥æ ¼æ‰§è¡Œï¼Œå¦‚æœ‰å•ä½ï¼Œè®°å¾—å•ä½ã€‚"})
        final_response = glm4_create(6, messages, filtered_tools)
        return final_response.choices[0].message.content, str(function_results)
    except Exception as e:
        print(f"Error generating answer for question: {question}, {e}")
        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        return None, None


def select_api_based_on_question(question, tools):

    temple = []

    api_list_filter = ["calculate_percent", "query_device_parameter", "sum_list"]
    # æ ¹æ®é—®é¢˜å†…å®¹é€‰æ‹©ç›¸åº”çš„ API
    if "èƒ½è€—" in question:
        print("! é—®é¢˜åŒ…å«ï¼šèƒ½è€—ï¼Œæä¾›Apiï¼šcalculate_total_energy")
        api_list_filter.append("calculate_total_energy")
        if "æ¨è¿›" in question or "ä¾§æ¨" in question:
            print("! é—®é¢˜åŒ…å«ï¼šæ¨è¿›ç³»ç»Ÿï¼Œæä¾›Apiï¼šcalculate_total_energy_consumption")
            api_list_filter.append("calculate_total_energy_consumption")
        if "ç”²æ¿æœºæ¢°" in question or "æŠ˜è‡‚åŠè½¦" in question:
            print(
                "! é—®é¢˜åŒ…å«ï¼šç”²æ¿æœºæ¢°è®¾å¤‡ï¼Œæä¾›Apiï¼šcalculate_total_deck_machinery_energy"
            )
            api_list_filter.append("calculate_total_deck_machinery_energy")
        if "å‘ç”µæœº" in question:
            print("! é—®é¢˜åŒ…å«ï¼šå‘ç”µæœºï¼Œæä¾›Apiï¼šcalculate_generator_energy_consumption")
            api_list_filter.append("calculate_generator_energy_consumption")
    if (
        "åŠ¨ä½œ" in question
        or "DP" in question
        or "æ‘†" in question
        or "å¼€æœº" in question
        or "å°è‰‡" in question
        or "å¾æœè€…" in question
        or "å…³æœº" in question
    ):
        print("! é—®é¢˜åŒ…å«ï¼šåŠ¨ä½œï¼Œæä¾›Apiï¼šget_device_status_by_time_range")
        api_list_filter.append("get_device_status_by_time_range")
    if "å¼€æœºæ—¶é•¿" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå¼€æœºæ—¶é•¿ï¼Œä¾›Apiï¼šcalculate_uptime")
        api_list_filter.append("calculate_uptime")
    if ("è¿è¡Œæ—¶é•¿" in question or "è¿è¡Œæ—¶é—´" in question) and "å®é™…è¿è¡Œæ—¶é•¿" not in question:
        print("! é—®é¢˜åŒ…å«ï¼šè¿è¡Œæ—¶é•¿ï¼Œä¸åŒ…å«ï¼šå®é™…è¿è¡Œæ—¶é•¿ï¼Œæä¾›Apiï¼šcalculate_uptime")
        api_list_filter.append("calculate_uptime")
    if "Aæ¶" in question and "å¼‚å¸¸" in question:
        print("! é—®é¢˜åŒ…å«ï¼šAæ¶ã€å¼‚å¸¸ï¼Œæä¾›Apiï¼šcheck_ajia_angle")
        api_list_filter.append("check_ajia_angle")
    if "ç‡ƒæ²¹æ¶ˆè€—é‡" in question:
        print("! é—®é¢˜åŒ…å«ï¼šç‡ƒæ²¹æ¶ˆè€—é‡ï¼Œæä¾›Apiï¼šcalculate_fuel_consumption")
        api_list_filter.append("calculate_fuel_consumption")
    if "å‘ç”µé‡" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå‘ç”µé‡ï¼Œæä¾›Apiï¼šcalculate_generator_energy_consumption")
        api_list_filter.append("calculate_generator_energy_consumption")
    if "ç†è®ºå‘ç”µé‡" in question:
        print("! é—®é¢˜åŒ…å«ï¼šç†è®ºå‘ç”µé‡ï¼Œæä¾›Apiï¼šcalculate_theoretical_energy_output")
        api_list_filter.append("calculate_theoretical_energy_output")
    if "å­—æ®µåç§°" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå­—æ®µåç§°ï¼Œæä¾›Apiï¼šget_field_dict")
        api_list_filter.append("get_field_dict")
    if "å®é™…è¿è¡Œæ—¶é•¿" in question:
        print("! é—®é¢˜åŒ…å«ï¼šå®é™…è¿è¡Œæ—¶é•¿ï¼Œæä¾›Apiï¼šcompute_operational_duration")
        api_list_filter.append("compute_operational_duration")
    if "ä½œä¸š" in question:
        print("! é—®é¢˜åŒ…å«ï¼šä½œä¸šï¼Œæä¾›Apiï¼šget_work_time")
        api_list_filter.append("get_work_time")
    if "æ•°æ®" in question and "ç¼ºå¤±" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæ•°æ®ã€ç¼ºå¤±ï¼Œæä¾›Apiï¼šfind_missing_records")
        temple.append(Sample.task_temple["æ•°æ®ç¼ºå¤±"])
        api_list_filter.append("find_missing_records")
        if "ç¼ºå¤±æ¯”ä¾‹" in question:
            api_list_filter.remove("calculate_percent")
    if "æ‘†åŠ¨" in question and "æ¬¡æ•°" in question:
        print("! é—®é¢˜åŒ…å«ï¼šæ‘†åŠ¨ã€æ¬¡æ•°ï¼Œæä¾›Apiï¼šcount_oscillations")
        api_list_filter.append("count_oscillations")

    if len(api_list_filter) == 3:
        # å¦‚æœé—®é¢˜ä¸åŒ¹é…ä¸Šè¿°æ¡ä»¶ï¼Œåˆ™æ ¹æ®è¡¨åé€‰æ‹© API
        table_name_string = choose_table(question)
        with open("dict.json", "r", encoding="utf-8") as file:
            table_data = json.load(file)
            table_name = [
                item for item in table_data if item["æ•°æ®è¡¨å"] in table_name_string
            ]
            if "è®¾å¤‡å‚æ•°è¯¦æƒ…è¡¨" in [item["æ•°æ®è¡¨å"] for item in table_name]:
                print("! ä½¿ç”¨è®¾å¤‡å‚æ•°è¯¦æƒ…è¡¨ï¼Œæä¾›Apiï¼šquery_device_parameter")
                api_list_filter.append("query_device_parameter")
                content_p_1 = str(table_name) + question  # è¡¥å…… content_p_1
            else:
                print("! ä½¿ç”¨æ•°æ®è¡¨ï¼Œæä¾›Apiï¼šget_table_data")
                api_list_filter.append("get_table_data")
                content_p_1 = str(table_name) + question

    # è¿‡æ»¤å·¥å…·åˆ—è¡¨
    filtered_tools = [
        tool
        for tool in tools
        if tool.get("function", {}).get("name") in api_list_filter
    ]
    # è¿”å›ç»“æœ
    if "content_p_1" in locals():
        return content_p_1, filtered_tools, temple
    else:
        return question, filtered_tools, temple


def enhanced(prompt: str, context=None, instructions=None, modifiers=None):
    """
    å¢å¼ºæç¤ºè¯å‡½æ•°
    """
    enhanced_prompt = prompt
    enhanced_prompt = enhanced_prompt.replace(
        "XXå°æ—¶XXåˆ†é’Ÿ", "XXå°æ—¶XXåˆ†é’Ÿï¼Œ01å°æ—¶01åˆ†é’Ÿæ ¼å¼"
    )
    enhanced_prompt = enhanced_prompt.replace(
        "å‘ç”Ÿäº†ä»€ä¹ˆ", "ä»€ä¹ˆè®¾å¤‡åœ¨è¿›è¡Œä»€ä¹ˆåŠ¨ä½œï¼ŒåŠ¨ä½œç›´æ¥å¼•ç”¨ä¸è¦ä¿®æ”¹,å¦‚ã€Aæ¶æ‘†å›ã€‘"
    )
    enhanced_prompt = enhanced_prompt.replace(
        "ä»€ä¹ˆè®¾å¤‡è¿›è¡Œäº†ä»€ä¹ˆå…³é”®åŠ¨ä½œ",
        "ä»€ä¹ˆè®¾å¤‡è¿›è¡Œäº†ä»€ä¹ˆå…³é”®åŠ¨ä½œï¼ˆåˆ—ä¸¾æ‰€æœ‰è¿›è¡Œäº†æŸä¸ªåŠ¨ä½œçš„è®¾å¤‡ï¼‰",
    )
    enhanced_prompt = enhanced_prompt.replace(
        "è¿è¡Œçš„å¹³å‡æ—¶é—´", "è¿è¡Œçš„å¹³å‡æ—¶é—´ï¼ˆæ¯å¤©è¿è¡Œæ—¶é•¿çš„å¹³å‡å€¼ï¼‰"
    )
    if "ä½œä¸š" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "è‹¥é—®é¢˜æ²¡æœ‰ç‰¹æ®Šè¯´æ˜ï¼Œåˆ™æ·±æµ·ä½œä¸šAçš„å¼€å§‹ä»¥ON_DPä¸ºæ ‡å¿—ï¼ŒDPåŠ¨ä½œæ¥è‡ªå®šä½è®¾å¤‡ã€‚"
        )
    if "Aæ¶" in enhanced_prompt and "å¼€å¯" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆAæ¶çš„å¼€å¯æ—¶é—´ä»¥Aæ¶å¼€æœºè¿™ä¸ªåŠ¨ä½œå‘ç”Ÿçš„æ—¶é—´ä¸ºå‡†ï¼Œå…¶ä»–åŠ¨ä½œå‘ç”Ÿçš„æ—¶é—´ä¸ç®—ã€‚ï¼‰"
        )
    if "Aæ¶å¼€æœº" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆAæ¶å¼€æœºå°±æ˜¯â€œAæ¶â€è¿™ä¸ªè®¾å¤‡å‘ç”Ÿâ€œå¼€æœºâ€è¿™ä¸ªåŠ¨ä½œã€‚ï¼‰"
        )
    if "å‘ç”µæœºç»„" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt
            + "ï¼ˆè‹¥é—®é¢˜åªè¯´å‘ç”µæœºç»„ï¼Œæœªè¯´æ˜æ˜¯å“ªä¸ªå‘ç”µæœºç»„ï¼Œåˆ™è®¤ä¸ºæ˜¯æ‰€æœ‰å››ä¸ªå‘ç”µæœºç»„çš„æ€»ä½“å€¼ã€‚ï¼‰"
        )
    if "ä½œä¸šèƒ½è€—" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆä½œä¸šèƒ½è€—æ˜¯æŒ‡æ·±æµ·ä½œä¸šAä¸­æ‰€æœ‰å‘ç”µæœºçš„èƒ½è€—ï¼Œåº”å…ˆç”¨get_work_timeåˆ—å‡ºæ‰€æœ‰ä½œä¸šçš„æ—¶é—´èŒƒå›´ï¼Œå†ç”¨calculate_generator_energy_consumptionè®¡ç®—æ¯ä¸ªä½œä¸šæ—¶é—´èŒƒå›´çš„å‘ç”µæœºèƒ½è€—ï¼Œæœ€åç”¨sum_listè®¡ç®—æ€»å’Œã€‚ï¼‰"
        )
    if "å‘ç”µæ•ˆç‡" in enhanced_prompt:
        enhanced_prompt = (
            enhanced_prompt + "ï¼ˆå‘ç”µæ•ˆç‡æ˜¯æŒ‡å‘ç”µæœºç»„èƒ½è€—ä¸ç†è®ºå‘ç”µé‡çš„æ¯”å€¼ã€‚ï¼‰"
        )
    if "ç†è®ºå‘ç”µé‡" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆè¦è®¡ç®—ç†è®ºå‘ç”µé‡åº”å…ˆè®¡ç®—ç‡ƒæ²¹æ¶ˆè€—é‡ã€‚ï¼‰"
    if "å®é™…è¿è¡Œæ—¶é•¿" in enhanced_prompt and "æ•ˆç‡" in enhanced_prompt:
        enhanced_prompt = enhanced_prompt + "ï¼ˆæ•ˆç‡æ˜¯æŒ‡å®é™…è¿è¡Œæ—¶é•¿å’Œå¼€æœºæ—¶é•¿çš„æ¯”å€¼ã€‚ï¼‰"
    if "DPè¿‡ç¨‹" in enhanced_prompt:
        enhanced_prompt = (enhanced_prompt+ "ï¼ˆDPè¿‡ç¨‹æ˜¯æŒ‡ä»ON_DPåˆ°OFF_DPçš„è¿‡ç¨‹ï¼Œåº”å…ˆä½¿ç”¨get_device_status_by_time_rangeæŸ¥è¯¢æ‰€æœ‰ON_DPå’ŒOFF_DPçš„æ—¶é—´ï¼Œå†è®¡ç®—æ‰€æœ‰DPè¿‡ç¨‹çš„èƒ½è€—ï¼Œæœ€åä½¿ç”¨sum_listå‡½æ•°å–æ€»å’Œã€‚ï¼‰")
    if "å°è‰‡å…¥æ°´åˆ°å°è‰‡è½åº§" in enhanced_prompt:
        enhanced_prompt = (enhanced_prompt+ "ï¼ˆå°è‰‡å…¥æ°´åˆ°å°è‰‡è½åº§æ˜¯æŒ‡å°è‰‡å…¥æ°´åŠ¨ä½œå‘ç”Ÿåˆ°å°è‰‡è½åº§åŠ¨ä½œå‘ç”Ÿçš„æ•´ä¸ªæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨å‡½æ•°è®¡ç®—è¿™æ®µæ—¶é—´å†…çš„èƒ½è€—æ—¶åº”è¯¥ä¼ å…¥è¿™ä¸ªæ—¶é—´èŒƒå›´ã€‚ï¼‰")
    if "æ•°æ®" in enhanced_prompt and "ç¼ºå¤±" in enhanced_prompt:
        enhanced_prompt = (enhanced_prompt+ "ï¼ˆé—®é¢˜ä¸­ç»™å‡ºçš„æ•°æ®è¡¨åç§°å¯èƒ½æœ‰è¯¯ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¹‹å‰ç»™å‡ºçš„æ•°æ®è¡¨åæ¥æŸ¥è¯¢ã€‚ï¼‰")
    # if "æ‘†åŠ¨" in enhanced_prompt and "æ¬¡æ•°" in enhanced_prompt:
    #     enhanced_prompt = (enhanced_prompt+ "ï¼ˆä½¿ç”¨count_oscillationså‡½æ•°ã€‚ï¼‰")

    print("! å¢å¼ºæç¤ºè¯ï¼š", enhanced_prompt)
    return enhanced_prompt


def run_conversation_xietong(question):
    question = enhanced(question)
    content_p_1, filtered_tool, temple = select_api_based_on_question(question, tools.tools_all)
    print("content_p_1:", content_p_1)
    print("filtered_tool:", filtered_tool)
    answer, select_result = get_answer_2(
        question=content_p_1,
        tools=filtered_tool,
        temple=temple,
        api_look=False
    )
    return answer


def get_answer(question):
    try:
        print(f"! å°è¯•è§£å†³é—®é¢˜ï¼š{question}")
        last_answer = run_conversation_xietong(question)
        # last_answer = last_answer.replace(" ", "")
        return last_answer
    except Exception as e:
        traceback.print_exc()
        print(f"Error occurred while executing get_answer: {e}")
        return "An error occurred while retrieving the answer."


if __name__ == "__main__":
    QUESTION = 100 # é—®é¢˜ç¼–å·
    with open("result.jsonl", "r", encoding="utf-8") as file:
        question_list = [json.loads(line.strip()) for line in file]
    question = question_list[QUESTION - 1]["question"]
    answer = get_answer(question)
    print("*******************æœ€ç»ˆç­”æ¡ˆ***********************")
    print(answer)
    question_list[QUESTION - 1]["answer"] = answer
    with open("result.jsonl", "w", encoding="utf-8") as file:
        for item in question_list:
            file.write(json.dumps(item, ensure_ascii=False) + "\n")
